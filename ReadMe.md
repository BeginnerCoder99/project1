# p3.py Headline Scraper + Sentiment Analysis
The program will read from a url_list.txt file any urls, one per line It will then scrape all h1, h2, and h3 files. Then it will put them all in a urlOutput.txt file, creating it if necessary. We use Selenium with headless browser to do website emulation and BeautifulSoup & lxml to parse everything. Finally we are using distilbert and Roberta AI models to give us feedback. The will read the urlOutput.txt file generated by scraper.py before creating AIoutput.txt file with positive, negative, or neutral responses to each headline with each AI model's response being separated.

## Installation
```
pip install beautifulsoup4 torch transformers lxml selenium pytest webdriver-manager
```

## Testing
You can use pytest to test the program if you make any changes.
```
pytest
```

## How to Use
Create a url_list.txt file with as many urls as you want, with each new url on a separate line. Make sure it is inside the same directory as scraper, sentiment, and main.py. Once that is done, simply run the main.py program. After running the program, the results should be saved to urlOutput.txt and AIoutput.txt

## Notes
It uses headless chrome to emulate the program. If a url has good bot protection, the program will not work. You might need to search for urls that are friendly to scraping or do not have good anti-bot security. If the website doesn't have an h1 header, it will timeout after 30 seconds. There is a dockerfile as well on github. By running the "docker build -t webscraper-sentiment ." command in terminal, you can build your own docker image. Make sure to run command "docker run --rm -v "%cd%:/app" webscraper-sentiment" in terminal after the image is built. This will save any files outputted by the image to your local directory.